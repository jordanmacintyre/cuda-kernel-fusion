# CUDA Kernel Fusion

Educational examples demonstrating **CUDA kernel fusion** to optimize GPU performance by reducing memory bandwidth bottlenecks. Achieves **2-3x speedup** over PyTorch by keeping intermediate values in GPU registers.

## What is Kernel Fusion?

**Problem:** Each PyTorch operation launches a separate kernel with expensive memory round-trips:

```python
# PyTorch - 3 kernels, 7 memory operations
a = x + y          # Kernel 1: read x,y ‚Üí write a
b = a * 2          # Kernel 2: read a ‚Üí write b
c = torch.exp(b)   # Kernel 3: read b ‚Üí write c
```

**Solution:** Single fused kernel keeps intermediates in registers:

```cuda
// Fused CUDA - 1 kernel, 3 memory operations
float a = x[idx] + y[idx];      // register (fast!)
float b = a * 2.0f;              // register (fast!)
output[idx] = expf(b);           // write to memory
```

**Result:** 2.3x speedup, 99% efficiency, < 1e-6 relative error

## Quick Start

```bash
# Setup
conda create -n cuda-fusion python=3.12
conda activate cuda-fusion
conda install pytorch pytorch-cuda=12.1 -c pytorch -c nvidia
conda install -c nvidia cuda-toolkit ninja
pip install pytest tqdm numpy

# Install package
pip install --no-build-isolation -e .

# Run tests and benchmarks
pytest tests/ -v
python benchmarks/bench_add_mul_exp.py
```

## Usage

```python
import torch
from ops.cuda import add_mul_exp, add_mul_exp_cuda
from ops.torch import add_mul_exp_pytorch

x = torch.randn(1_000_000, device='cuda')
y = torch.randn(1_000_000, device='cuda')

# Main API (dispatches to CUDA)
result = add_mul_exp(x, y)

# Or explicitly use CUDA implementation
result_cuda = add_mul_exp_cuda(x, y)

# PyTorch baseline for comparison
result_pytorch = add_mul_exp_pytorch(x, y)
```

**Naming Convention:**
- `operation_cuda()` - CUDA implementation (fused)
- `operation_pytorch()` - PyTorch baseline (unfused)
- `operation_triton()` - Triton implementation (future)
- `operation()` - Main API (dispatches to best available)

## Performance

**RTX 3070, 10M elements:**
```
PyTorch (unfused):   0.686ms
CUDA (fused):        0.297ms
Speedup:             2.31x (99.2% efficiency)
Max relative error:  8.13e-07 ‚úì
```

**Why is it faster?**

Modern GPUs are **memory-bound**. Memory access is 400x slower than register operations.

| Version | Memory Operations | Total Traffic | Time |
|---------|------------------|---------------|------|
| PyTorch | 7 (read x,y,a,b; write a,b,c) | 267 MB | 0.686ms |
| CUDA Fused | 3 (read x,y; write c) | 114 MB | 0.297ms |
| **Savings** | **4 fewer ops (57%)** | **153 MB (57%)** | **2.3x faster** |

Intermediate values `a` and `b` stay in registers ‚Üí **no memory round-trips**.

## Project Structure

```
cuda-kernel-fusion/
‚îú‚îÄ‚îÄ ops/                           # Multi-backend operations
‚îÇ   ‚îú‚îÄ‚îÄ cuda/                     # CUDA implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ add_mul_exp.py       # Python wrapper + add_mul_exp_cuda()
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ csrc/                # CUDA source files
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ add_mul_exp.cu   # CUDA kernel
‚îÇ   ‚îú‚îÄ‚îÄ torch/                    # PyTorch reference implementations
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ add_mul_exp.py       # add_mul_exp_pytorch()
‚îÇ   ‚îî‚îÄ‚îÄ triton/                   # (future) Triton implementations
‚îú‚îÄ‚îÄ tests/                         # 19 tests (correctness, accuracy, edge cases)
‚îÇ   ‚îî‚îÄ‚îÄ test_add_mul_exp.py
‚îú‚îÄ‚îÄ benchmarks/                    # Performance analysis
‚îÇ   ‚îú‚îÄ‚îÄ utils/                    # Reusable benchmarking framework
‚îÇ   ‚îú‚îÄ‚îÄ bench_add_mul_exp.py      # Example benchmark
‚îÇ   ‚îî‚îÄ‚îÄ bench_template.py         # Template for new kernels
‚îú‚îÄ‚îÄ pyproject.toml                # Config & dependencies
‚îî‚îÄ‚îÄ README.md
```

## Testing

```bash
# Run all 19 tests
pytest tests/ -v

# Skip slow tests (16 tests)
pytest tests/ -m "not slow"

# Run only numerical accuracy tests
pytest tests/ -m "numerical"
```

**Test coverage:**
- ‚úÖ Correctness (basic ops, known values, various sizes)
- ‚úÖ Input validation (CPU tensors, shape/dtype checks)
- ‚úÖ Edge cases (zeros, large values, numerical stability)
- ‚úÖ Accuracy (< 1e-5 relative error)

## Benchmarking

```bash
# Run comprehensive benchmark
python benchmarks/bench_add_mul_exp.py
```

**Output includes:**
- ‚è±Ô∏è Detailed timing with warmup and statistics
- üî¨ Numerical accuracy analysis
- üìä Memory bandwidth calculations
- üß™ PyTorch profiler kernel-level details

See `benchmarks/README.md` for details on the benchmarking framework.

## Adding New Operations

### 1. Create CUDA Kernel

`ops/cuda/csrc/your_op.cu`:
```cuda
#include <torch/extension.h>

__global__ void your_op_kernel(const float* x, const float* y,
                                float* output, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        // Your fused operations here
        float temp = x[idx] + y[idx];
        output[idx] = temp * 2.0f;
    }
}

torch::Tensor your_op_cuda(torch::Tensor x, torch::Tensor y) {
    auto output = torch::empty_like(x);
    int threads = 256;
    int blocks = (x.numel() + threads - 1) / threads;

    your_op_kernel<<<blocks, threads>>>(
        x.data_ptr<float>(), y.data_ptr<float>(),
        output.data_ptr<float>(), x.numel()
    );

    return output;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("your_op_cuda", &your_op_cuda);
}
```

### 2. Create Python Wrapper and Baseline

`ops/cuda/your_op.py`:
```python
import torch
from torch.utils.cpp_extension import load

# Compile CUDA extension
_C = load(name="your_op_cuda", sources=["csrc/your_op.cu"], ...)

def your_op_cuda(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
    """CUDA implementation."""
    if not x.is_cuda or not y.is_cuda:
        raise ValueError("Inputs must be CUDA tensors")
    return _C.your_op(x, y)

def your_op(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
    """Main API - dispatches to CUDA."""
    return your_op_cuda(x, y)
```

`ops/torch/your_op.py`:
```python
import torch

def your_op_pytorch(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
    """PyTorch baseline implementation."""
    # Your unfused PyTorch code here
    temp = x + y
    return temp * 2
```

### 3. Add Tests and Benchmarks

```bash
# Copy templates
cp tests/test_add_mul_exp.py tests/test_your_op.py
cp benchmarks/bench_template.py benchmarks/bench_your_op.py

# Edit and run
pytest tests/test_your_op.py -v
python benchmarks/bench_your_op.py
```

## Key CUDA Concepts

**Thread Indexing:**
```cuda
int idx = blockIdx.x * blockDim.x + threadIdx.x;
```

**Kernel Launch:**
```cuda
int threads = 256;  // Threads per block
int blocks = (size + threads - 1) / threads;  // Ceiling division
my_kernel<<<blocks, threads>>>(args);
```

**Memory Hierarchy (latency):**
- Registers: 1 cycle ‚Üê Keep data here!
- Shared memory: ~20 cycles
- Global memory: ~400 cycles ‚Üê Avoid round-trips

## Operations That Benefit from Fusion

1. **Activation functions:** `sigmoid(x * w + b)`, `gelu(x)`, `swish(x)`
2. **Normalization:** `layer_norm(x)`, `rms_norm(x)`
3. **Element-wise chains:** `(x + y) * z + w`, `dropout(relu(x))`

## Troubleshooting

| Error | Solution |
|-------|----------|
| `nvcc not found` | `conda install -c nvidia cuda-toolkit` |
| `ninja not found` | `conda install ninja` |
| Slow compilation | Set `export TORCH_CUDA_ARCH_LIST='8.6'` |
| Import error | Run `pip install --no-build-isolation -e .` |
| Tests fail | Ensure `torch.cuda.is_available()` returns True |

## Resources

- üìñ [CUDA C Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)
- üîß [PyTorch Custom Extensions](https://pytorch.org/tutorials/advanced/cpp_extension.html)
- üí° [NVIDIA CUDA Samples](https://github.com/NVIDIA/cuda-samples)
- üìä [NVIDIA Nsight Compute](https://developer.nvidia.com/nsight-compute)

## License

MIT - See LICENSE file

---

**Author:** Jordan MacIntyre
**Purpose:** Educational demonstration of CUDA kernel fusion techniques
**GPU Tested:** RTX 3070 (compute capability 8.6)
